Program started running at: 1606585970.717566
2015-06-16 15:20:40+00:00    Big time in you.S. today - MAKE AMERICA GREAT ...
2015-06-16 15:24:41+00:00                      Thanks. https://t.co/eZfgrOy1Hn
2015-06-16 16:00:39+00:00    "@AppSame We are going to listen @realDonaldTr...
2015-06-16 16:04:05+00:00    It is almost time. I will be making a major an...
2015-06-16 16:07:50+00:00    Make sure to follow me on @periscopeco #MakeAm...
Name: noContractions, dtype: object
2015-06-16 15:20:40+00:00    [big, time, in, you, ., s, ., today, -, make, ...
2015-06-16 15:24:41+00:00                 [thanks, ., https://t.co/ezfgroy1hn]
2015-06-16 16:00:39+00:00    [", @appsame, we, are, going, to, listen, @rea...
2015-06-16 16:04:05+00:00    [it, is, almost, time, ., i, will, be, making,...
2015-06-16 16:07:50+00:00    [make, sure, to, follow, me, on, @periscopeco,...
Name: loweredTokens, dtype: object
2015-06-16 15:20:40+00:00    [big, time, ., ., today, -, make, america, gre...
2015-06-16 15:24:41+00:00                 [thanks, ., https://t.co/ezfgroy1hn]
2015-06-16 16:00:39+00:00    [", @appsame, going, listen, @realdonaldtrump,...
2015-06-16 16:04:05+00:00    [almost, time, ., making, major, announcement,...
2015-06-16 16:07:50+00:00    [make, sure, follow, @periscopeco, #makeameric...
Name: noStopWordsTokens, dtype: object
2015-06-16 15:20:40+00:00    [big, time, today, make, america, great, polit...
2015-06-16 15:24:41+00:00                    [thanks, https://t.co/ezfgroy1hn]
2015-06-16 16:00:39+00:00    [@appsame, going, listen, @realdonaldtrump, re...
2015-06-16 16:04:05+00:00    [almost, time, making, major, announcement, @t...
2015-06-16 16:07:50+00:00    [make, sure, follow, @periscopeco, #makeameric...
Name: lemmatizedTokens, dtype: object
['happy', 'veteran', 'day', 'served', 'thank', 'special', 'work', 'pathetic', 'attempt', '@foxnews', 'try', 'build', 'rating', '#gopdebate', 'without', 'would', 'rating', 'https://t.co/2bx54vkpqh', 'matter', 'https://t.co/sdfmhgneaf', 'pundit', 'honest', 'hopefully', 'looking', 'strong', 'great', 'country', 'make', 'strong', 'great', 'job', 'republican', 'party', 'must', 'get', 'tougher', 'smarter', 'fast', 'go', 'big', 'defeat', 'like', 'last', 'two', 'time', 'great', 'day', 'united', 'state', 'america', 'great', 'plan', 'repeal', 'replace', 'obamacare', '‚Ä¶', 'https://t.co/0tnmln55lm', 'made', 'speech', 'arkansas', 'last', 'night', 'record', 'gop', 'crowd', 'great', 'spirit', 'amazing', 'people', 'make', 'america', 'great', 'united', 'state', 'cannot', 'continue', 'make', 'bad', 'one-sided', 'trade', 'deal', 'many', 'job', 'give', '@danielhalper', 'great', 'job', '@cnn', 'today', 'wise', 'indeed', 'allowed', 'run', 'guilty', 'hell', 'nice', 'lost', 'campaigned', 'wrong', 'state']
['veteran', 'day', 'work', 'attempt', 'rating', '#gopdebate', 'rating', 'matter', 'https://t.co/sdfmhgneaf', 'pundit']
[('great', 695), ('thank', 589), ('hillary', 363), ('people', 310), ('#trump2016', 308), ('america', 293), ('trump', 290), ('new', 275), ('make', 270), ('poll', 255), ('#makeamericagreatagain', 251), ('clinton', 225), ('get', 224), ('big', 223), ('job', 210), ('time', 185), ('‚Ä¶', 164), ('country', 163), ('crooked', 161), ('would', 160), ('today', 160), ('going', 150), ('president', 146), ('like', 145), ('many', 141), ('vote', 139), ('bad', 137), ('want', 137), ('medium', 136), ('join', 130), ('u', 130), ('last', 129), ('back', 127), ('...', 123), ('night', 121), ('one', 118), ('never', 118), ('cruz', 117), ('state', 114), ('said', 114), ('debate', 110), ('news', 109), ('year', 109), ('tonight', 108), ('crowd', 107), ('say', 107), ('american', 106), ('support', 106), ('iowa', 104), ('let', 104)]
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1 entries, 0 to 0
Columns: 8254 entries, #1for to üö®
dtypes: int64(8254)
memory usage: 64.5 KB
None
   #1for  #1in  #2a  #abc2020  #ahca  #alisareal  ...  üá¶  üáµ   üá∏   üá∫  üèà  üö®
0      1     1    2         1      1           1  ...  1  1  24  24  1  2

[1 rows x 8254 columns]
            Word  Count
3185       great    695
7610       thank    589
3337     hillary    363
6306      people    310
205   #trump2016    308
[('thank', 372), ('poll', 232), ('clinton', 216), ('job', 210), ('trump', 205), ('time', 185), ('country', 163), ('today', 160), ('president', 146), ('medium', 129), ('night', 121), ('america', 118), ('join', 118), ('state', 114), ('vote', 112), ('news', 109), ('year', 109), ('debate', 107), ('#makeamericagreatagain', 102), ('tomorrow', 102), ('support', 98), ('campaign', 98), ('deal', 93), ('day', 92), ('way', 88), ('border', 82), ('crowd', 81), ('election', 80), ('watch', 80), ('story', 78), ('word', 69), ('cruz', 69), ('money', 65), ('show', 65), ('thing', 64), ('speech', 63), ('tax', 63), ('hampshire', 60), ('morning', 60), ('rating', 59), ('bush', 59), ('nothing', 59), ('candidate', 57), ('let', 57), ('ad', 57), ('woman', 55), ('jeb', 54), ('rubio', 54), ('everyone', 53), ('number', 53)]
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1 entries, 0 to 0
Columns: 4433 entries, #amazon to ‚ùå
dtypes: int64(4433)
memory usage: 34.6 KB
None
   #amazon  #americanunity  #americasmerkel  #asktrump  #becool  ...  ‚Äú  ‚Äù  ‚Ä¢   ‚Ä¶  ‚ùå
0        1               1                1          1        1  ...  6  5  1  17  1

[1 rows x 4433 columns]
         Word  Count
4118    thank    372
3487     poll    232
652   clinton    216
2893      job    210
4210    trump    205
Firsts ten entries to popularNounsList: ['thank', 'poll', 'clinton', 'job', 'trump', 'time', 'country', 'today', 'president', 'medium']
Firsts ten entries to popularWordsList: ['great', 'thank', 'hillary', 'people', '#trump2016', 'america', 'trump', 'new', 'make', 'poll']
Index(['truncated', 'text', 'is_quote_status', 'favorite_count', 'retweeted',
       'retweet_count', 'allCaps', 'exclamationPoints', 'hashtags',
       'userHandleCount',
       ...
       'pella', 'manhattan', 'everyday', 'manchester', 'proof', 'gov.kasich',
       'forgiveness', 'impressive', 'headquarters', 'misleading'],
      dtype='object', length=2512)
                           truncated  ... misleading
2015-11-11 19:34:53+00:00        0.0  ...          0
2016-01-27 01:43:27+00:00        0.0  ...          0
2015-08-18 18:14:36+00:00        0.0  ...          0
2016-03-27 23:55:59+00:00        0.0  ...          0
2015-07-20 00:03:06+00:00        0.0  ...          0
2017-05-05 05:55:02+00:00        1.0  ...          0
2015-07-18 16:49:36+00:00        0.0  ...          0
2016-03-28 04:17:57+00:00        0.0  ...          0
2015-12-25 05:22:42+00:00        0.0  ...          0
2017-01-13 14:25:54+00:00        0.0  ...          0

[10 rows x 2512 columns]
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 4191 entries, 2015-11-11 19:34:53+00:00 to 2016-01-23 02:08:31+00:00
Columns: 2512 entries, truncated to misleading
dtypes: float64(5), int64(2505), object(2)
memory usage: 80.3+ MB
None
volatilityUp     0.0   1.0
Cluster Labels            
0               2343  1845
1                  1     2
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 4191 entries, 2015-11-11 19:34:53+00:00 to 2016-01-23 02:08:31+00:00
Columns: 2512 entries, truncated to kmeansClusterLabel_1
dtypes: float32(5), int8(2505), uint8(2)
memory usage: 10.1 MB
                           text
2015-06-16 07:00:00+00:00     2
2015-06-16 08:00:00+00:00     3
2015-06-16 09:00:00+00:00     5
2015-06-16 10:00:00+00:00     1
2015-06-16 11:00:00+00:00     0
          text
Hour          
0     0.951904
1     0.434202
2     0.133601
3     0.083500
4     0.517702

BASELINE LOGISTIC CLASSIFICATION
ACCURACY SCORE:

0.5525876460767947

BASELINE LOGISTIC CLASSIFICATION
AUC SCORES:

0.548063973063973

BASELINE LOGISTIC CLASSIFICATION
CONFUSION MATRIX:

[[601 404]
 [400 392]]

BASELINE LOGISTIC CLASSIFICATION
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

         0.0       0.60      0.60      0.60      1005
         1.0       0.49      0.49      0.49       792

    accuracy                           0.55      1797
   macro avg       0.55      0.55      0.55      1797
weighted avg       0.55      0.55      0.55      1797

SVD selection for number of features: {'svd__estimator__n_components': 88}

LOGISTIC CLASSIFICATION WITH SVD
ACCURACY SCORE:

0.5525876460767947

LOGISTIC CLASSIFICATION WITH SVD
AUC SCORES:

0.5763794663048394

LOGISTIC CLASSIFICATION WITH SVD
CONFUSION MATRIX:

[[879 126]
 [657 135]]

LOGISTIC CLASSIFICATION WITH SVD
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

         0.0       0.57      0.87      0.69      1005
         1.0       0.52      0.17      0.26       792

    accuracy                           0.56      1797
   macro avg       0.54      0.52      0.47      1797
weighted avg       0.55      0.56      0.50      1797

TUNED LOGISTIC CLASSIFICATION HYPERPARAMETERS: {'logReg__estimator__C': 0.00026366508987303583, 'logReg__estimator__l1_ratio': 0.030303030303030304}
Best score is 0.5595323311858745

TUNED LOGISTIC CLASSIFICATION
ACCURACY SCORE:

0.560378408458542

TUNED LOGISTIC CLASSIFICATION
AUC SCORE:

0.5686874968591386

TUNED LOGISTIC CLASSIFICATION
CONFUSION MATRIX:

[[1002    3]
 [ 787    5]]

TUNED LOGISTIC CLASSIFICATION
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

         0.0       0.56      1.00      0.72      1005
         1.0       0.62      0.01      0.01       792

    accuracy                           0.56      1797
   macro avg       0.59      0.50      0.36      1797
weighted avg       0.59      0.56      0.41      1797

TUNED DECISION TREE CLASSIFIER HYPERPARAMETERS: {'decisionTree__estimator__min_samples_split': 36, 'decisionTree__estimator__max_depth': 4, 'decisionTree__estimator__criterion': 'gini'}
Best score is 0.5366261035552374

TUNED DECISION TREE CLASSIFIER
ACCURACY SCORE:

0.55370061213133

TUNED DECISION TREE CLASSIFIER
AUC SCORE:

0.5332592090054776

TUNED DECISION TREE CLASSIFIER
CONFUSION MATRIX:

[[808 197]
 [605 187]]

TUNED DECISION TREE CLASSIFIER
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

         0.0       0.57      0.80      0.67      1005
         1.0       0.49      0.24      0.32       792

    accuracy                           0.55      1797
   macro avg       0.53      0.52      0.49      1797
weighted avg       0.53      0.55      0.51      1797

TUNED RANDOM FOREST CLASSIFIER HYPERPARAMETERS: {'randomForest__estimator__max_features': 'sqrt', 'randomForest__estimator__n_estimators': 400}
Best score is 0.5335242185635887

TUNED RANDOM FOREST CLASSIFIER
ACCURACY SCORE:

0.5709515859766278

TUNED RANDOM FOREST CLASSIFIER
AUC SCORE:

0.5724213528318006

TUNED RANDOM FOREST CLASSIFIER
CONFUSION MATRIX:

[[850 155]
 [616 176]]

TUNED RANDOM FOREST CLASSIFIER
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

         0.0       0.58      0.85      0.69      1005
         1.0       0.53      0.22      0.31       792

    accuracy                           0.57      1797
   macro avg       0.56      0.53      0.50      1797
weighted avg       0.56      0.57      0.52      1797

TUNED ADABOOST CLASSIFIER HYPERPARAMETERS: {'adaBoost__estimator__n_estimators': 200}
Best score is 0.5211166785969935

TUNED ADABOOST CLASSIFIER
ACCURACY SCORE:

0.5158597662771286

TUNED ADABOOST CLASSIFIER
AUC SCORE:

0.5139875119352731

TUNED ADABOOST CLASSIFIER
CONFUSION MATRIX:

[[635 370]
 [500 292]]

TUNED ADABOOST CLASSIFIER
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

         0.0       0.56      0.63      0.59      1005
         1.0       0.44      0.37      0.40       792

    accuracy                           0.52      1797
   macro avg       0.50      0.50      0.50      1797
weighted avg       0.51      0.52      0.51      1797

TUNED STOCHASTIC GRADIENT BOOSTED DECISION TREE CLASSIFIER HYPERPARAMETERS: {'gradientBoost__estimator__n_estimators': 10, 'gradientBoost__estimator__subsample': 0.8999999999999999}
Best score is 0.5547602004294918

TUNED STOCHASTIC GRADIENT BOOSTED DECISION TREE CLASSIFIER
ACCURACY SCORE:

0.5548135781858653

TUNED STOCHASTIC GRADIENT BOOSTED DECISION TREE CLASSIFIER
AUC SCORE:

0.548324036383738

TUNED STOCHASTIC GRADIENT BOOSTED DECISION TREE CLASSIFIER
CONFUSION MATRIX:

[[982  23]
 [777  15]]

TUNED STOCHASTIC GRADIENT BOOSTED DECISION TREE CLASSIFIER
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

         0.0       0.56      0.98      0.71      1005
         1.0       0.39      0.02      0.04       792

    accuracy                           0.55      1797
   macro avg       0.48      0.50      0.37      1797
weighted avg       0.49      0.55      0.41      1797

The following weights will be applied to the vote of each model (in order): [0.20335218093699514, 0.20092891760904685, 0.20718901453957997, 0.18719709208400645, 0.20133279483037153]

MODEL ENSEMBLE VOTING CLASSIFIER
ACCURACY SCORE:

0.55370061213133

MODEL ENSEMBLE VOTING CLASSIFIER
AUC SCORE:

0.5653211216644052

MODEL ENSEMBLE VOTING CLASSIFIER
CONFUSION MATRIX:

[[942  63]
 [733  59]]

MODEL ENSEMBLE VOTING CLASSIFIER
CLASSIFICATION REPORT:

              precision    recall  f1-score   support

         0.0       0.56      0.94      0.70      1005
         1.0       0.48      0.07      0.13       792

    accuracy                           0.56      1797
   macro avg       0.52      0.51      0.42      1797
weighted avg       0.53      0.56      0.45      1797

Program ended at: 1606589614.1348534
Total program running time: 3643.417287349701
