---
title: 'Tweet or Honk: Has Trump been Goosing the Markets with Social Media?'
layout: post
---
<img src="{{ 'assets/images/trumpVolatilityMostPopularNouns.jpeg' | relative_url }}" style="max-width:100%;"><br>
<!-- Text stuff -->
<p><h2>Abstract:</h2></p>
<p>In this post we use Trump’s Twitter data to predict whether market volatility (the VIX) will increase that day. To accomplish this we use a Natural Language Processing (NLP) bag-of-words approach with dimensionality reduction through truncated Singular Value Decomposition (SVD) for latent sentiment analysis. The predictions are decided by a weighted voting classifier ensemble consisting of hyperparameter tuned logistic regression, decision tree, random forest, Adaptive Boosting (AdaBoost) of decision trees, and stochastic gradient boosted decision tree models. To handle the large amounts of data we use Dask and sklearn’s joblib for parallel processing as well as sparse matrix data structures. </p>
<hr />
<p><h2>Background:</h2></p>
<p>To say that President Trump has garnered public attention with his Twitter posts is a gross understatement; one need only look at the <a href="https://www.washingtonpost.com/news/get-there/wp/2017/06/01/would-you-buy-a-21-99-covfefe-t-shirt/" target="_blank" rel="noopener noreferrer">small fortunes being made selling ‘covfefe’ merch on the internet</a> as an indicator of the rapt attention he holds both over his supporters and detractors alike. He is at the same time Jack Dorsey’s greatest enemy and his greatest asset by <a href="https://www.reuters.com/article/us-usa-twitter-trump/trump-complained-to-twitter-ceo-about-lost-followers-source-idUSKCN1RZ2D1" target="_blank" rel="noopener noreferrer">drawing the president’s personal ire for his actions</a> and creating a mountain of ad-space revenue in one push of a blue button. Though Obama had a Twitter account as during his administration, it served as a more traditional and benign presence. Even before declaring his intentions to run for office in June of 2015 Trump communicated with Twitter in a way that was direct, unpolished, seemingly always irate, and at times obscene. Now it has become almost expected as a new standard: Can any future president really be seen as engaged with their constituency if their every unfiltered thought is not constantly being uploaded to the clout-cloud?</p>
<p>In this light it has become a common inside joke among investors that they live in constant fear of what his next Tweet will bring to the market. This is not to say other presidents and governmental actions have not affected the market. In an age where securities can be bought and sold in microseconds and a <a href="https://www.bbc.com/news/world-us-canada-54880774" target="_blank" rel="noopener noreferrer">tweet can be shared with the general public before even the president’s own staff find out</a>, there can seem to be some evidence for this claim.</p>
<p>Investors can however be a notoriously superstitious bunch; regardless of how many Fibonacci retracements and moving averages you want to throw in a chart, technical “analysis” has very little to do with quantitative mathematics and <a href="https://www.youtube.com/watch?v=5zvvJ8hAJN0" target="_blank" rel="noopener noreferrer">more to do with engagement</a> (besides, backtesting reveals it is <a href="https://www.youtube.com/watch?v=PMrGwKlXMTk" target="_blank" rel="noopener noreferrer">about as effective as flipping a coin</a>). So how do we know if there is any merit to this claim?</p>
<p>To address the question of whether Trump has any power to affect markets with his use of social media we first need to look at our problem through a greater scope. In a perfectly efficient market both everyone and no one participating in the market has this power of influence to some roughly equal extent. Everyone has this power by their own valuations determining where they lie on the bid ask spread. At the same time no one has this power as it surpasses no one else’s ability to do the same, and if you have actual intentions of buying a product your bid needs to be at least reasonable. In reality, markets aren’t quite so efficient, and some people hold more sway over others. It only makes sense that a person that holds power over financial and fiscal policies would hold at least a little more of this power than others.</p>
<p>We must also consider where this information actually comes from. While Trump may be the author of the tweet, is he the originator of the idea that can cause changes in the market, or is he merely a conduit for that idea? He is, after all, an elective representative. Isn’t it logical that, to at least some extent, his tweets represent the opinions of a large segment of the market participants?</p>
<p>One might logically conclude that the market is more worried about the action that may follow a particular tweet than the tweet itself, but while policies are rigid and binding with an ability to set real limits on profits, tweets are quite the opposite. They are temporary documents of opinions that may change within minutes, and in general operate on a more “sell the rumor, buy the news” principle. If Trump’s tweets really have a consistent relationship with the markets we should expect to see spikes of volatility correlated to our tweet’s metrics. As implied volatility tends to overstate historical volatility, we should expect to see the most reaction to our tweet from volatility change rather than price movement of underlying assets. In addition, the VIX gives us an ability to get a two-sided perspective on volatility. Although it is usually associated with falling prices, a large spike in price can also drive volatility.</p>
<p>In this project we will use our data science tools to see to what extent, if any, Trump’s use of Twitter has on market volatility.</p>
<hr />
<p><h2>Computing Environment:</h2></p>
Python 3.7.0<br>
numpy 1.19.2<br>
pandas 1.1.2<br>
dask 2.30.0<br>
dask-ml 1.7.0<br>
yfinance 0.1.55<br>
multiprocess 0.70.10<br>
joblib 0.17.0<br>
DateTime 4.3<br>
matplotlib 3.3.2<br>
seaborn 0.11.0<br>
wordcloud 1.8.0<br>
Pillow 7.2.0<br>
scipy 1.5.3<br>
contractions 0.0.25<br>
nltk 3.5<br>
scikit-learn 0.23.2<br>
<hr />
<p><h2>Data:</h2></p>
<p><h3>Twitter Data:</h3> The following columns are derived from Donald Trump’s Twitter data relating to the verified account <a href="https://twitter.com/realDonaldTrump" target="_blank" rel="noopener noreferrer">@realDonaldTrump</a>.<br>
<b>‘truncated’:</b> This column indicates whether the tweet is truncated. A truncated tweet is a tweet that exceeds the maximum allowed characters for a tweet and is divided among two or more tweets with the use of an ellipsis at the end and beginning of the continuing tweets to indicate this to the user. A 1 indicates the tweet being part of a truncated body while a 0 indicates the entire message is in a single tweet.<br>
<b>‘text’:</b> This column is a string containing all the words in the tweet, including emojis.<br>
<b>‘Is_quote_status’:</b> This column indicates whether this tweet is a quote of another user’s tweet.<br>
<b>‘favorite_count’:</b> This column counts how many other users have favorited this tweet.<br>
<b>‘retweeted’:</b>  This column indicates whether this tweet has been retweeted by other users. A 1 indicates that the tweet has been retweeted by other users. A 0 indicates the tweet has not been retweeted by other users.<br>
<b>‘retweet_count’:</b> This column counts how many other users have retweeted this tweet.</p>
<p><h3>Market Data:</h3><br>
<b>^VIX:</b> The CBOE Volatility Index used to measure forward-looking (implied) volatility. Calculated from the call to put ratio on 23 to 37 day options contracts on SPX.</p>
<hr />
<p><h2>Data sourcing and cleaning:</h2></p>
<p>JSON files containing Trump’s Twitter data were sourced from <a href="https://github.com/bpb27/trump_tweet_data_archive" target="_blank" rel="noopener noreferrer">https://github.com/bpb27/trump_tweet_data_archive</a>. This data gives us all of Trump’s tweets up until about March of 2017. Unfortunately, the code used to generate this data has become deprecated. I spent quite some time working with Selenium and BeautifulSoup to try and rectify this in addition to communicating with the publisher as well as other Github users who had commented on this same issue, but to know avail. It seems there are two main issues: the CSS selector has been changed to some very obscure location, and Twitter will stop your page requests after a few hundred pages even with a delay of five seconds between requests. I also tried to source the data directly from Twitter using the Tweepy API, but my free Twitter Developer Access only allows me to go a few thousand tweets back in time. For full access to a user’s newsfeed one would need to pay for a higher tier Twitter Developer Account, and would be the most likely plan of action were this to be deployed.</p>
<p>To establish proof of concept, we downloaded three JSON files from the Github archive for the years 2015, 2016, and 2017 (up to deprecation). As we loaded our unstructured JSON files into Pandas DataFrames, some columns became dictionaries containing what were other variables. Some of the data tracked by Twitter was added and removed throughout the years, so we rectify our files to only contain data that is also in the other years’ files. We drop all columns except for those mentioned in the data dictionary above.</p>
<p>Our next step takes into consideration when we define a day to be. If Trump tweets something after markets are closed, it obviously should not be counted as relating to that same calendar day. In respect to this we should set our ‘day’ to start and end at 4pm when markets close. Since our data is already in UTC (as all good data time series data is stored) and five hours ahead of Eastern Time, we shift all of our tweets three hours ahead so that 4pm Eastern time becomes our new ‘midnight’.</p>
<p>Our VIX data was scraped using the yfinance Yahoo Finance API. We retrieved all historical data for ^VIX closing prices. We then localized the time zone to UTC from a date only object. We next define a variable called ‘volatilityUp’ that is 1 if the volatility is as high or higher than the previous close and 0 if not. We chose to count volatility as up even if the price is exactly the same as it was the day before. This is because volatility inherently depreciates over time as front-end contracts are rolled out for more expensive later-dated contracts in addition to normal contract theta-decay. To put it simply, if there were no inherent risk-free rate in the market and volatility could grow in the long-term there wouldn’t be a market because no one would invest in it (at least in the long-term, people still buy lottery tickets). If volatility is at the same level today as it was yesterday despite these factors that is essentially a growth so we are going to count that as an up day (though the scenario where something closes at the exact same price as it did the day before almost never happens). After creating this column, we drop the original column that contained the closing prices so that we have only our ‘volatilityUp’ variable and our datetime index. If this model were to be scaled for deployment, there are paid data sources that can provide hourly, minute, and ticker level data.</p>
<p>After performing our initial data cleaning on our individual table we append each year’s DataFrame in consecutive order and delete duplicate rows to account for any overlap in file entries (there is overlap, so this is necessary). Next we merge our Twitter DataFrame with our VIX DataFrame with a full join. We drop all rows occurring before July 16th of 2015 to limit our data to after the day Trump announced his intentions to run for office. Since there is only one closing price per day and turning our VIX date object to a datetime object sets each close price at midnight of that day, we forward fill our ‘volatilityUp’ values to fill each tweet with the volatility outcome for that market day. We next drop all rows where our ‘text’ value is empty. We now have one remaining issue with our data. The Twitter historical data for a user contains not only tweets authored by that user, but all tweets that mention that user’s twitter handle. Luckily, these all appear with the same beginning format: @USERNAME: (where USERNAME would be filled by the tweet authors username). We use a simple Regex statement to filter these other users out.</p>
<p>Next we create some new variables from our text data. All-caps words indicate emphasis, and is widely seen as the internet equivalent to yelling. There are some common exceptions to this (USA, FOX, UK, and NAFTA to name a few), but a high presence of capitalized words may indicate heightened emotion, or personal volatility. We use a Regex statement to count the number of all-caps words per tweet. Exclamation points also indicate higher emotional levels in a message. We use a Regex statement to count the number of exclamation points per tweet. We count the total number of words in a tweet, as being able to fit a higher number of words within the same character limit indicates a high presence of shorter words. This may be an indicator of a tweet being written more impulsively with less thought put to the concept beforehand.</p>
<p>We next use Regex statements to create two more variables. The first counts the number of hashtags per tweet. Hashtags are often utilized in social media to raise attention to certain subjects, and users’ tweets can be reverse searched by which hashtags are mentioned. If a user includes a high number of hashtags in their tweet this may indicate their wish for others to recognize their perspective regarding the hashtag subject and imply some sort of call to action around the subject. The second variable counts the number of user handle mentions for similar reasons. In addition, mentioning other users directly can be a form of confrontation, which may imply more personal volatility (as in the colloquial: “don’t @ me, bro”).</p>
<hr />
<p><h2>Natural Language Preprocessing:</h2></p>
<p>All of this cleaning will be applied to the text contained in the ‘text’ column. First we change all contractions to their base for (i.e. “can’t” becomes “can not”. This prevents our tokenizer from splitting words and removing their context (i.e. “can’t” becomes “can” , “‘“, and “t”, yielding “can” instead of it’s negation).</p>
<p>Next we tokenize our words. Doing this to unconventional text like tweets would normally be a nightmare, but luckily the creators of the nltk package have built a TweetTokenizer that takes into account things like hashtags and user handles as well as emojis. When we tokenize a word we take one string that contains all of our tweet text and return a list of strings containing the individual words for this tweet.</p>
<p>After this we convert all of our words into lowercase. This prevents our program from judging “Failing” and “failing” as two different words. We then remove all of our stop words (i.e. “and”, “the”, “a”) in order to remove noise from our data. We can also remove all punctuation marks at this point. Next we lemmatize our words. Lemmatizing converts all words to the singular present tense (i.e. “runs” and “ran” become “run”, “rockets” becomes “rocket”) so they are counted under a single entry. There are some misprints from this process: for example "Kansas" becomes "kansa" and "ISIS" becomes "isi".</p>
<hr />
<p><h2>Training and Test Data Split:</h2></p>
<p>Before we continue our feature engineering, we must split our data into training and test sets. For example, we will eventually want to make a column for each unique word in a tweet. If we deploy this model in the future we would not be able to create a column for new data for any unique word not previously used in to create the model. By the same logic we should only engineer non-universal columns for our model using our training data. The same holds true for our variables created from unsupervised learning.</p>
<p>First we create our X and Y data sets holding our independent variables respectively. In this case our dependent variable is ‘volatilityUp’, and our independent variables are the data we are using to predict market volatility. We randomly sample 70% of our data for training and save the remaining 30% for testing our models. We choose to perform stratified random sampling based on our ‘volatilityUp’ variable so that we have equal proportions in both the test and training sets. This is not only because of the size of the skew between volatility entries, but the inherent nature of this skew due to volatility’s constant depreciation as discussed previously. The remaining feature engineering is derived solely from the training data.</p>
<img src="{{ 'assets/images/trumpVolatilityCountTweetsVolatilityUp.jpeg' | relative_url }}" style="max-width:100%;"><br>
<hr />
<p><h2>NLP for Feature Engineering:</h2></p>
<p>We created a new DataFrame containing a count of all the unique words in all of our tweets. We refine this DataFrame to the 2,500 most commonly used words of the 8,270 words used in all of the tweets. We then create a new column in our original DataFrame for each of the 2,500 most commonly used unique words and in that column count the number of times that word occurs for each individual tweet. Next we drop our lemmatized tokens column as well as our original text as they are no longer needed.</p>
<p>This approach most closely resembles a bag-of-words NLP strategy. On the other hand, tf-idf (term frequency - inverse document frequency) uses the inverse log to punish the counts of words within a single document if they also commonly appear in all documents. The general idea is that a word carries less sentiment the more often it is used. Tweets are highly temporal, and to me this does not account for this. Let’s say we have an extreme hypothetical scenario where a Twitter user who is running for political office threatens to imprison their political opponent upon election to office (unrealistic, I know). They may say this many times, and the words used to indicate this sentiment may eventually be used so many times that the audience becomes desensitized to them. However, due to the high frequency of tweet generation, I personally hold that it doesn’t matter as much how many times the words were used as how many times the words have been used so far. If this candidate mentions locking up their opponent the first few times, the public may react with shock, it may be in the news, and the market may react. After you cry wolf a few times, is the public going to react the same? Probably not. To me this indicates more of a need for Bayesian inference for penalizing our word counts (as in increasing the count penalty for a word the more it has appeared so far). I do not know if this approach is established and it goes a bit beyond the scope of this project so I decided to use a straight word count instead.</p>
<p>At this point our training data contains 4191 observations (tweets) and 2512 columns (not including the datetime index), consuming a little over 80.3 MB of memory.</p>
<hr />
<p><h2>Unsupervised Cluster Analysis for Feature Engineering:</h2></p>
<p>Next we used unsupervised K-means clustering to create new variables.This analysis uses Euclidean distances between variables to find a mean distance between measurements and group data into clusters based on the desired number (k) of clusters based on centroid points of these distances. This allows us to group our observations as relative to one another as opposed to the dependent variable.</p>
<p>We use a silhouette score to gauge the appropriate number of clusters to set. The silhouette score measures the distance of observations from all clusters other than the one to which they are designated. A score closer to 1 indicates a larger distance between our clusters, meaning we have more well defined clusters. A score closer to zero indicates undefined clusters. Scores closer to -1 indicate that observations have been selected for the wrong clusters. We then iterate through between 2 and 10 clusters and check our silhouette scores.</p>
<img src="{{ 'assets/images/trumpVolatilityVaryingInertiaKmeans.jpeg' | relative_url }}" style="max-width:100%;"><br>
<p>We observe that our silhouette score is closest to 1 when we set 2 clusters. We fit our model for two clusters and assign the cluster designations to a new column. We then use our existing model based on our training data to predict cluster assignment on the test data and create a new column for this set. The cluster assignments are the categories as opposed to numeric data, so we transform this column in both sets to a dummy variable (only 1 as there are two categories). To predict on our model we require the same number of columns in the training and the test sets. When creating clusters we may have created columns in our training set that do not exist in our test set (this is not possible with only 2 clusters, but is in the instance where k is determined to be higher). To remedy this we check which columns are missing in our test data, and add that column filled with a zero value if that is the case</p>
<p>d add that column filled with a zero value if that is the case. 
	Our data is now starting to get pretty large. In order to save memory we downcast all of our int64s to in8s and our float64s to float32s. This reduces the memory of our training data to around 10.1 MB.</p>
<hr />
<p><h2>Exploratory Data Analysis:</h2></p>
<p>To start off  I want to answer some of my most pressing personal questions. The first to come to mind is: How often is Trump tweeting per day? We resample all of our data to a daily frequency and count our observations per day. Most days range between 3 and 20 tweets with a spike near his candidacy announcement and his largest spike with around 80 tweets in a day falling around the proceeding election day.</p>
<img src="{{ 'assets/images/trumpVolatilityCountTrumpTweetsPerDayLineplot.jpeg' | relative_url }}" style="max-width:100%;"><br>
<img src="{{ 'assets/images/trumpVolatilityCountTrumpTweetsPerDayPDF.jpeg' | relative_url }}" style="max-width:100%;"><br>
<p>Looking at a probability distribution function of tweets per day shows a relatively lognormal distribution with a few extreme outliers.</p>
<p>We also want to know what time of day Trump normally tweets at. He is often depicted as tweeting late into the night. If this is the case we might expect the market to have more time to react then correct before the next open. We push our data back 8 hours to go back to calendar time from market time, resample data to 60 minutes, then create a probability distribution function to see what percent of the time tweets for each hour.</p>
<img src="{{ 'assets/images/trumpVolatilityPercentTrumpTweetsPerHourLineplot.jpeg' | relative_url }}" style="max-width:100%;"><br>
<p>We also want to know what time of day Trump normally tweets at. He is often depicted as tweeting late into the night. If this is the case we might expect the market to have more time to react then correct before the next open. We push our data back 8 hours to go back to calendar time from market time, resample data to 60 minutes, then create a probability distribution function to see what percent of the time tweets for each hour. 
It seems that public perception of him may be a bit skewed in this case. Not too high of a percent of tweets occur past 11 PM, but there are noteworthy outliers at late hours. His activity seems instead to peak at 8 AM, with a second peak at 4 PM and a third from 8 to 11 PM. It is worth noting that this third peak occurs while FOX News airs Tucker Carlson Tonight, Hannity, and The Ingraham Angle. The president often calls in to these shows and maintains a dialogue with these television personalities via Twitter.</p>
<p>We also make histograms of probability distribution functions from the regex variables we created. These all seem to exhibit a lognormal distribution.</p>
<img src="{{ 'assets/images/trumpVolatilityExclamationPointsPerTrumpTweet.jpeg' | relative_url }}" style="max-width:100%;"><br>
<img src="{{ 'assets/images/trumpVolatilityAllCapsPerTweet.jpeg' | relative_url }}" style="max-width:100%;"><br>
<img src="{{ 'assets/images/trumpVolatilityUserHandleMentionsPerTweet.jpeg' | relative_url }}" style="max-width:100%;"><br>
<img src="{{ 'assets/images/trumpVolatilityUserHandleMentionsPerTweet.jpeg' | relative_url }}" style="max-width:100%;"><br>
<p>Next we want to examine our word frequency within all of our tweets. Because of the large number of unique words, a bar chart would be prohibitive beyond the top few words. We instead create a word cloud with the size of the word dependent on the count of that word’s occurrence. Oh yeah, they’re also shaped like Trump’s head. A word cloud of the top 2500 commonly used words is generated using a free image (<a href="https://www.freeimg.net/photo/868386/trump-donaldtrump-president-usa" target="_blank" rel="noopener noreferrer">https://www.freeimg.net/photo/868386/trump-donaldtrump-president-usa</a>).</p>
<img src="{{ 'assets/images/trumpVolatilityMostPopularWords.jpeg' | relative_url }}" style="max-width:100%;"><br>
<p>The title image was created by taking the 2500 most popular nouns and another free image (<a href="https://commons.wikimedia.org/wiki/File:Donald_Trump_by_Gage_Skidmore_5.jpg" target="_blank" rel="noopener noreferrer">https://commons.wikimedia.org/wiki/File:Donald_Trump_by_Gage_Skidmore_5.jpg</a>). Tutorials for how to fit a wordcloud to an image can be found <a href="https://amueller.github.io/word_cloud/auto_examples/parrot.html" target="_blank" rel="noopener noreferrer">here</a>.</p>
<hr />
<p><h2>Working with Dask and memory requirements:</h2></p>
<p>Although we previously reduced the memory needed for our DataFrame to 10.1 MB, we will be performing some more memory intensive model fitting and hyperparameter tuning that will make this add up quickly. Dask allows us to preserve memory in two ways. Using Dask data structures like arrays and DataFrames allows us to partition our data between our CPU cores. In this case we create 2 workers for each core, giving us eight partitions in total. Secondly, Dask allows us to parallelize our processes. Once data is stored in a Dask data structure any processes performed on the structure until it reaches a point where it is instructed to calculate (in our case the calculation is our model fit using ParallelPostFit functions). Dask then takes all the processes that are required up until the calculation and optimizes them by running processes in parallel among our workers where possible. This in turn saves us in memory usage and time required to run our program.</p>
<p>A natural question that may arise is why we chose this point to parallelize our data structures instead of just doing it from the start and avoiding Pandas altogether. Parallelization can save memory when implemented correctly on large amounts of data. On modest sized data the time consumed partitioning the data and parallelizing processes will actually make Dask slower than serial operations. Relatively speaking, the size of our data isn’t all that large until we engineer a column for the top 2,500 unique words in all of our text. After we do this we have greatly increased our number of data points, and I felt like this was an appropriate point to implement parallelization.</p>
<p>One would probably consider implementing parallelization from the beginning if this model were to be scaled up for deployment. In this case we would probably have the funding to use a higher tier Twitter API to access every tweet up to the present instead of the hard files of limited data that we are using in this project. Handling large amounts of unstructured JSON data at that scale would also be much easier in a Dask bag than treating it as nested dictionaries in a Pandas DataFrame as we have done in this project.</p>
<p>To parallelize our data structures we simply convert all of our Pandas DataFrames (both the dependent and independent training and test sets) into Dask DataFrames. We then convert these Dask DataFrames to Dask Arrays, which work similarly to NumPy arrays and further reduce memory requirements when model fitting.</p>
<p>There is one more trick we have up our sleeve to save memory and time. Our array has 2,500 columns that contain counts of unique words. Considering a tweet only has space for 250 characters and we aren’t counting common filler words like participles, that leaves us with a very large amount of data points that contain the value zero for the times a word is <b>not</b> mentioned in a tweet. This type of data is known as ‘sparse’ (as opposed to ‘dense’). Our value of zero is still a value and matters for our models, but in terms of memory this is a lot of redundant information and takes up the majority of the memory needed for our array. We take a shortcut around this by converting our Dask arrays into compressed sparse row (CSR)  matrices. We know that we are going to be using matrix dot multiplication in our models, and that the product of anything and zero is equal to zero. Therefore, we are wasting a lot of memory when we make a model calculate a data point where we already know the answer. Instead of using the memory to hold a value of zero, we essentially fill in a NA value anywhere there is a zero in the original array. Then we tell our array that any calculation performed on that point will yield a zero. This saves us memory and time (and keeps the program from crashing!). We have done this after converting over to Dask, so these data structure conversions are delayed as well.</p>
<hr />
<p><h2>Dimension Reduction with Truncated Singular Value Decomposition:</h2></p>
